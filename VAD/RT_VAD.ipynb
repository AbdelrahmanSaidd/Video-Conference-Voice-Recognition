{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d6GQL2VpayV",
        "outputId": "68728172-f7d6-44f6-d8cc-51f4d0069787"
      },
      "outputs": [],
      "source": [
        "!pip install numpy==1.20.2\n",
        "!pip install torch==1.9.0\n",
        "!pip install matplotlib==3.4.2\n",
        "!pip install torchaudio==0.9.0\n",
        "!pip install soundfile==0.10.3.post1\n",
        "!pip install pyaudio==0.2.11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLEGVgm0t6hH",
        "outputId": "9feaf197-03ab-4e27-b76c-1fa9e2b96c6e"
      },
      "outputs": [],
      "source": [
        "!apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg\n",
        "!pip install pyaudio\n",
        "!pip install torch\n",
        "!pip install torchaudio\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install jupyterplot==0.0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LSnri0ttfBH",
        "outputId": "73b4729d-f6e7-483a-c8c5-8f07d35daf3b"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import numpy as np\n",
        "import torch\n",
        "torch.set_num_threads(1)\n",
        "import torchaudio\n",
        "import matplotlib\n",
        "import matplotlib.pylab as plt\n",
        "torchaudio.set_audio_backend(\"soundfile\")\n",
        "import pyaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPTWEUrdvtP-",
        "outputId": "7107ce6e-cc2d-4aeb-f794-2ea2a39930c7"
      },
      "outputs": [],
      "source": [
        "model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
        "                              model='silero_vad',\n",
        "                              force_reload=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "EW4U1v4RvwZM"
      },
      "outputs": [],
      "source": [
        "(get_speech_timestamps,\n",
        " save_audio,\n",
        " read_audio,\n",
        " VADIterator,\n",
        " collect_chunks) = utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EDquR24zvyW9"
      },
      "outputs": [],
      "source": [
        "# Taken from utils_vad.py\n",
        "def validate(model,\n",
        "             inputs: torch.Tensor):\n",
        "    with torch.no_grad():\n",
        "        outs = model(inputs)\n",
        "    return outs\n",
        "\n",
        "# Provided by Alexander Veysov\n",
        "def int2float(sound):\n",
        "    abs_max = np.abs(sound).max()\n",
        "    sound = sound.astype('float32')\n",
        "    if abs_max > 0:\n",
        "        sound *= 1/32768\n",
        "    sound = sound.squeeze()  # depends on the use case\n",
        "    return sound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "r47eNJdtv0hN"
      },
      "outputs": [],
      "source": [
        "FORMAT = pyaudio.paInt16\n",
        "CHANNELS = 1\n",
        "SAMPLE_RATE = 16000\n",
        "CHUNK = int(SAMPLE_RATE / 10)\n",
        "\n",
        "audio = pyaudio.PyAudio()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "k4BVjG3Rv2CN",
        "outputId": "ba5c0e86-6210-4103-ab74-288c697e5d9a"
      },
      "outputs": [],
      "source": [
        "num_samples = 1536\n",
        "stream = audio.open(format=FORMAT,\n",
        "                    channels=CHANNELS,\n",
        "                    rate=SAMPLE_RATE,\n",
        "                    input=True,\n",
        "                    frames_per_buffer=CHUNK)\n",
        "data = []\n",
        "voiced_confidences = []\n",
        "\n",
        "print(\"Started Recording\")\n",
        "for i in range(0, 100):\n",
        "\n",
        "    audio_chunk = stream.read(num_samples)\n",
        "\n",
        "    # in case you want to save the audio later\n",
        "    data.append(audio_chunk)\n",
        "\n",
        "    audio_int16 = np.frombuffer(audio_chunk, np.int16);\n",
        "\n",
        "    audio_float32 = int2float(audio_int16)\n",
        "\n",
        "    # get the confidences and add them to the list to plot them later\n",
        "    new_confidence = model(torch.from_numpy(audio_float32), 16000).item()\n",
        "    voiced_confidences.append(new_confidence)\n",
        "\n",
        "print(\"Stopped the recording\")\n",
        "\n",
        "# plot the confidences for the speech\n",
        "plt.figure(figsize=(20,6))\n",
        "plt.plot(voiced_confidences)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "from jupyterplot import ProgressPlot\n",
        "import threading\n",
        "\n",
        "continue_recording = True\n",
        "\n",
        "def stop():\n",
        "    input(\"Press Enter to stop the recording:\")\n",
        "    global continue_recording\n",
        "    continue_recording = False\n",
        "\n",
        "def start_recording():\n",
        "\n",
        "    stream = audio.open(format=FORMAT,\n",
        "                    channels=CHANNELS,\n",
        "                    rate=SAMPLE_RATE,\n",
        "                    input=True,\n",
        "                    frames_per_buffer=CHUNK)\n",
        "\n",
        "    data = []\n",
        "    voiced_confidences = []\n",
        "\n",
        "    global continue_recording\n",
        "    continue_recording = True\n",
        "\n",
        "    pp = ProgressPlot(plot_names=[\"Silero VAD\"],line_names=[\"speech probabilities\"], x_label=\"audio chunks\")\n",
        "\n",
        "    stop_listener = threading.Thread(target=stop)\n",
        "    stop_listener.start()\n",
        "\n",
        "    while continue_recording:\n",
        "\n",
        "        audio_chunk = stream.read(num_samples)\n",
        "\n",
        "        # in case you want to save the audio later\n",
        "        data.append(audio_chunk)\n",
        "\n",
        "        audio_int16 = np.frombuffer(audio_chunk, np.int16);\n",
        "\n",
        "        audio_float32 = int2float(audio_int16)\n",
        "\n",
        "        # get the confidences and add them to the list to plot them later\n",
        "        new_confidence = model(torch.from_numpy(audio_float32), 16000).item()\n",
        "        voiced_confidences.append(new_confidence)\n",
        "\n",
        "        pp.update(new_confidence)\n",
        "\n",
        "\n",
        "    pp.finalize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start_recording()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
